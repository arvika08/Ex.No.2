# Ex.No: 2  
## Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta  

**DATE: 19.08.2025**  
**REGISTER NUMBER: 212223060023**  

---

## Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a prompt-based output using different prompting tools of 2024.  

---

## AI Tools Required:
- OpenAI ChatGPT  
- Anthropic Claude  
- Google Bard (Gemini)  
- Cohere Command  
- Meta AI  

---

## Explanation:
1. **Define the Use Case**  
   A text summarization and technical Q&A task was chosen as the common use case. This allows testing across accuracy, clarity, depth, and relevance.  

2. **Create a Set of Prompts**  
   - *Prompt 1:* Summarize the following paragraph into 3 lines.  
   - *Prompt 2:* Explain the difference between supervised and unsupervised learning.  
   - *Prompt 3:* Generate a short creative story about AI in education.  

3. **Run the Experiment on Each AI Platform**  
   - Each prompt was tested on ChatGPT, Claude, Bard, Cohere Command, and Meta.  
   - Responses were collected under the same conditions.  
   - Noted: response speed, quality, clarity, and ease of use.  

4. **Evaluate Response Quality**  
   - **Accuracy**: Correctness of information.  
   - **Clarity**: Ease of understanding.  
   - **Depth**: Level of detail and explanation.  
   - **Relevance**: How closely the response matched the prompt.  

---

## Output (Comparison Table):

| **Criteria**   | **ChatGPT** | **Claude** | **Bard (Gemini)** | **Cohere Command** | **Meta AI** |
|----------------|-------------|------------|-------------------|---------------------|-------------|
| **Accuracy**   | 9/10 – Highly accurate in technical Q&A | 9/10 – Very precise and factually strong | 8/10 – Sometimes gives extra but less consistent | 7/10 – Good but limited in technical depth | 7/10 – Mostly correct but sometimes shallow |
| **Clarity**    | 9/10 – Very clear and well-structured | 10/10 – Extremely human-like, conversational | 8/10 – Clear but less structured | 7/10 – Sometimes short and vague | 7/10 – Simple but not always polished |
| **Depth**      | 9/10 – Detailed with examples | 9/10 – Strong contextual explanations | 8/10 – Good depth but can be surface-level | 6/10 – Lacks deep analysis | 6/10 – More generic |
| **Relevance**  | 9/10 – Direct and relevant answers | 9/10 – Highly contextual | 8/10 – Relevant but slightly verbose | 7/10 – Relevant but not focused | 6/10 – Sometimes off-topic |
| **Response Time** | Fast | Medium-Fast | Fast | Fast | Medium |
| **Ease of Use** | Excellent (UI + API) | Excellent (UI) | Good (Google ecosystem) | Good (Developer focused) | Good (Integrated with apps) |

---

## Conclusion:
- **ChatGPT** and **Claude** performed the best overall in terms of accuracy, clarity, and depth.  
- **Bard (Gemini)** was strong in creativity but slightly inconsistent in technical precision.  
- **Cohere Command** is more suited for developers needing concise outputs, but it lacks depth.  
- **Meta AI** is improving but still trails behind in detail and precision.  

For **technical Q&A**, ChatGPT and Claude are the most reliable.  
For **creative generation**, Bard is a strong choice.  
For **developer-centric lightweight tasks**, Cohere Command works well.  
Meta AI is best for **basic, casual usage**.  

---

## References:
1. OpenAI. (2024). *ChatGPT Documentation*. [https://platform.openai.com/](https://platform.openai.com/)  
2. Anthropic. (2024). *Claude AI*. [https://www.anthropic.com/](https://www.anthropic.com/)  
3. Google DeepMind. (2024). *Gemini / Bard*. [https://bard.google.com/](https://bard.google.com/)  
4. Cohere. (2024). *Cohere Command R Documentation*. [https://cohere.com/](https://cohere.com/)  
5. Meta AI. (2024). *Meta AI Research*. [https://ai.meta.com/](https://ai.meta.com/)  

---

## Result:
The prompt for the above problem statement executed successfully. The performance of five AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) was evaluated, and their outputs were compared based on accuracy, clarity, depth, and relevance.  
