# Ex.No: 2  
## Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta  


**REGISTER NUMBER: 212223060023**  

---

## Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a prompt-based output using different prompting tools of 2024.  

---

## AI Tools Required:
- OpenAI ChatGPT  
- Anthropic Claude  
- Google Bard (Gemini)  
- Cohere Command  
- Meta AI  

---

## Explanation:
1. **Define the Use Case**  
   A text summarization and technical Q&A task was chosen as the common use case. This allows testing across accuracy, clarity, depth, and relevance.  

2. **Create a Set of Prompts**  
   - *Prompt 1:* Summarize the following paragraph into 3 lines.  
   - *Prompt 2:* Explain the difference between supervised and unsupervised learning.  
   - *Prompt 3:* Generate a short creative story about AI in education.  

3. **Run the Experiment on Each AI Platform**  
   - Each prompt was tested on ChatGPT, Claude, Bard, Cohere Command, and Meta.  
   - Responses were collected under the same conditions.  
   - Noted: response speed, quality, clarity, and ease of use.  

4. **Evaluate Response Quality**  
   - **Accuracy**: Correctness of information.  
   - **Clarity**: Ease of understanding.  
   - **Depth**: Level of detail and explanation.  
   - **Relevance**: How closely the response matched the prompt.  

---

## Prompt 1: Summarization

Paragraph to Summarize:

“Machine learning is a subset of artificial intelligence that enables systems to learn from data rather than explicit programming. Over time, these systems can improve performance on tasks like image recognition, speech processing, and predictive analytics. However, challenges remain in ensuring model interpretability, avoiding bias, and requiring large amounts of high-quality labeled data.”

Outputs:

### ChatGPT:

Machine learning lets systems learn from data instead of being hand-coded.

It powers applications like image recognition, speech processing, and predictive analytics.

Major challenges include interpretability, bias, and the need for large, good-quality labeled datasets.

### Claude:

Machine learning enables systems to improve through data rather than fixed rules.

It’s used for tasks such as recognizing images, processing speech, and making predictions.

Key obstacles are lack of interpretability, bias in models, and dependence on large labeled datasets.

### Bard (Gemini):
• Machine learning means models learn from examples rather than explicit code.
• Common uses include image/speech recognition and predictive analytics.
• Challenges: explaining model decisions, bias issues, and obtaining sufficient labeled data.

### Cohere Command:

Machine learning lets systems learn from data and improve.

Common tasks: image and speech recognition, prediction.

Problems: interpretability, bias, big need for labeled data.

### Meta AI:
• Machine learning uses data instead of fixed instructions.
• Applications include recognition and prediction tasks.
• Main issues: model interpretability, bias, and needing lots of high-quality data.

## Prompt 2: Technical Q&A

Question: Explain the difference between supervised and unsupervised learning.

Outputs:

### ChatGPT:
Supervised learning uses labeled data (input + correct output) to train models for classification/regression.
Unsupervised learning uses unlabeled data, discovering hidden patterns (clusters, associations, dimensionality reduction).
Evaluation differs: supervised has accuracy/error vs. unsupervised uses internal metrics.

### Claude:
Supervised learning: input features + known labels → model learns mapping.
Unsupervised learning: no labels, model groups or finds structure in data.
Key differences: label presence, tasks, evaluation methods, and data requirements.

### Bard (Gemini):
Supervised learning: trained on labeled examples, good for tasks with ground truth (classification).
Unsupervised learning: trained on data without labels, useful for clustering, exploration, anomaly detection.

### Cohere Command:
Supervised = labeled training data, straightforward evaluation.
Unsupervised = unlabeled data, finds patterns, more exploratory, harder to measure success.

### Meta AI:
Supervised learning maps inputs → outputs with labels.
Unsupervised learning detects clusters or patterns without labels.
Difference: data type, task, and evaluation.

## Prompt 3: Creative Writing

Task: Generate a short creative story about AI in education.

Outputs:

### ChatGPT:
A school introduced an AI tutor “Mira” who adapted lessons for each student. Mira turned algebra into games, explained history as stories, and gave encouragement. Learning became joyful, and students discovered confidence alongside knowledge.

### Claude:
At Greenwood Academy, AI “Selene” sang fractions, told history as tales, and encouraged shy students. The class didn’t just learn academics—they learned courage and curiosity.

### Bard (Gemini):
At Sunrise High, AI “EKI” rolled into classrooms. It helped Luis design games with math and Priya write poetry with rhythm. Learning became an adventure where curiosity was the compass.

### Cohere Command:
Maplewood School’s AI “Aurora” created mini-games, linked art to history, and gave students confidence. By year’s end, education felt personal and inspiring.

### Meta AI:
AI mentor “BrightMind” guided students with diagrams and stories. Mistakes became learning steps, and students discovered belief in themselves with AI by their side.

## Output (Comparison Table):

| **Criteria**   | **ChatGPT** | **Claude** | **Bard (Gemini)** | **Cohere Command** | **Meta AI** |
|----------------|-------------|------------|-------------------|---------------------|-------------|
| **Accuracy**   | 9/10 – Highly accurate in technical Q&A | 9/10 – Very precise and factually strong | 8/10 – Sometimes gives extra but less consistent | 7/10 – Good but limited in technical depth | 7/10 – Mostly correct but sometimes shallow |
| **Clarity**    | 9/10 – Very clear and well-structured | 10/10 – Extremely human-like, conversational | 8/10 – Clear but less structured | 7/10 – Sometimes short and vague | 7/10 – Simple but not always polished |
| **Depth**      | 9/10 – Detailed with examples | 9/10 – Strong contextual explanations | 8/10 – Good depth but can be surface-level | 6/10 – Lacks deep analysis | 6/10 – More generic |
| **Relevance**  | 9/10 – Direct and relevant answers | 9/10 – Highly contextual | 8/10 – Relevant but slightly verbose | 7/10 – Relevant but not focused | 6/10 – Sometimes off-topic |
| **Response Time** | Fast | Medium-Fast | Fast | Fast | Medium |
| **Ease of Use** | Excellent (UI + API) | Excellent (UI) | Good (Google ecosystem) | Good (Developer focused) | Good (Integrated with apps) |

---

## Conclusion:
- **ChatGPT** and **Claude** performed the best overall in terms of accuracy, clarity, and depth.  
- **Bard (Gemini)** was strong in creativity but slightly inconsistent in technical precision.  
- **Cohere Command** is more suited for developers needing concise outputs, but it lacks depth.  
- **Meta AI** is improving but still trails behind in detail and precision.  

For **technical Q&A**, ChatGPT and Claude are the most reliable.  
For **creative generation**, Bard is a strong choice.  
For **developer-centric lightweight tasks**, Cohere Command works well.  
Meta AI is best for **basic, casual usage**.  

---

## References:
1. OpenAI. (2024). *ChatGPT Documentation*. [https://platform.openai.com/](https://platform.openai.com/)  
2. Anthropic. (2024). *Claude AI*. [https://www.anthropic.com/](https://www.anthropic.com/)  
3. Google DeepMind. (2024). *Gemini / Bard*. [https://bard.google.com/](https://bard.google.com/)  
4. Cohere. (2024). *Cohere Command R Documentation*. [https://cohere.com/](https://cohere.com/)  
5. Meta AI. (2024). *Meta AI Research*. [https://ai.meta.com/](https://ai.meta.com/)  

---

## Result:
The prompt for the above problem statement executed successfully. The performance of five AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) was evaluated, and their outputs were compared based on accuracy, clarity, depth, and relevance.  
